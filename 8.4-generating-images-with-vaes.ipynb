{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farid/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating images\n",
    "\n",
    "This notebook contains the second code sample found in Chapter 8, Section 4 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Variational autoencoders\n",
    "\n",
    "\n",
    "Variational autoencoders, simultaneously discovered by Kingma & Welling in December 2013, and Rezende, Mohamed & Wierstra in January 2014, \n",
    "are a kind of generative model that is especially appropriate for the task of image editing via concept vectors. They are a modern take on \n",
    "autoencoders -- a type of network that aims to \"encode\" an input to a low-dimensional latent space then \"decode\" it back -- that mixes ideas \n",
    "from deep learning with Bayesian inference.\n",
    "\n",
    "A classical image autoencoder takes an image, maps it to a latent vector space via an \"encoder\" module, then decode it back to an output \n",
    "with the same dimensions as the original image, via a \"decoder\" module. It is then trained by using as target data the _same images_ as the \n",
    "input images, meaning that the autoencoder learns to reconstruct the original inputs. By imposing various constraints on the \"code\", i.e. \n",
    "the output of the encoder, one can get the autoencoder to learn more or less interesting latent representations of the data. Most \n",
    "commonly, one would constraint the code to be very low-dimensional and sparse (i.e. mostly zeros), in which case the encoder acts as a way \n",
    "to compress the input data into fewer bits of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Autoencoder](https://s3.amazonaws.com/book.keras.io/img/ch8/autoencoder.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In practice, such classical autoencoders don't lead to particularly useful or well-structured latent spaces. They're not particularly good \n",
    "at compression, either. For these reasons, they have largely fallen out of fashion over the past years. Variational autoencoders, however, \n",
    "augment autoencoders with a little bit of statistical magic that forces them to learn continuous, highly structured latent spaces. They \n",
    "have turned out to be a very powerful tool for image generation.\n",
    "\n",
    "A VAE, instead of compressing its input image into a fixed \"code\" in the latent space, turns the image into the parameters of a statistical \n",
    "distribution: a mean and a variance. Essentially, this means that we are assuming that the input image has been generated by a statistical \n",
    "process, and that the randomness of this process should be taken into accounting during encoding and decoding. The VAE then uses the mean \n",
    "and variance parameters to randomly sample one element of the distribution, and decodes that element back to the original input. The \n",
    "stochasticity of this process improves robustness and forces the latent space to encode meaningful representations everywhere, i.e. every \n",
    "point sampled in the latent will be decoded to a valid output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VAE](https://s3.amazonaws.com/book.keras.io/img/ch8/vae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In technical terms, here is how a variational autoencoder works. First, an encoder module turns the input samples `input_img` into two \n",
    "parameters in a latent space of representations, which we will note `z_mean` and `z_log_variance`. Then, we randomly sample a point `z` \n",
    "from the latent normal distribution that is assumed to generate the input image, via `z = z_mean + exp(z_log_variance) * epsilon`, where \n",
    "epsilon is a random tensor of small values. Finally, a decoder module will map this point in the latent space back to the original input \n",
    "image. Because `epsilon` is random, the process ensures that every point that is close to the latent location where we encoded `input_img` \n",
    "(`z-mean`) can be decoded to something similar to `input_img`, thus forcing the latent space to be continuously meaningful. Any two close \n",
    "points in the latent space will decode to highly similar images. Continuity, combined with the low dimensionality of the latent space, \n",
    "forces every direction in the latent space to encode a meaningful axis of variation of the data, making the latent space very structured \n",
    "and thus highly suitable to manipulation via concept vectors.\n",
    "\n",
    "The parameters of a VAE are trained via two loss functions: first, a reconstruction loss that forces the decoded samples to match the \n",
    "initial inputs, and a regularization loss, which helps in learning well-formed latent spaces and reducing overfitting to the training data.\n",
    "\n",
    "Let's quickly go over a Keras implementation of a VAE. Schematically, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input into a mean and variance parameter\n",
    "z_mean, z_log_variance = encoder(input_img)\n",
    "\n",
    "# Draw a latent point using a small random epsilon\n",
    "z = z_mean + exp(z_log_variance) * epsilon\n",
    "\n",
    "# Then decode z back to an image\n",
    "reconstructed_img = decoder(z)\n",
    "\n",
    "# Instantiate a model\n",
    "model = Model(input_img, reconstructed_img)\n",
    "\n",
    "# Then train the model using 2 losses:\n",
    "# a reconstruction loss and a regularization loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the encoder network we will use: a very simple convnet which maps the input image `x` to two vectors, `z_mean` and `z_log_variance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "img_shape = (28, 28, 1)\n",
    "batch_size = 16\n",
    "latent_dim = 2  # Dimensionality of the latent space: a plane\n",
    "\n",
    "input_img = keras.Input(shape=img_shape)\n",
    "\n",
    "x = layers.Conv2D(32, 3,\n",
    "                  padding='same', activation='relu')(input_img)\n",
    "x = layers.Conv2D(64, 3,\n",
    "                  padding='same', activation='relu',\n",
    "                  strides=(2, 2))(x)\n",
    "x = layers.Conv2D(64, 3,\n",
    "                  padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(64, 3,\n",
    "                  padding='same', activation='relu')(x)\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for using `z_mean` and `z_log_var`, the parameters of the statistical distribution assumed to have produced `input_img`, to \n",
    "generate a latent space point `z`. Here, we wrap some arbitrary code (built on top of Keras backend primitives) into a `Lambda` layer. In \n",
    "Keras, everything needs to be a layer, so code that isn't part of a built-in layer should be wrapped in a `Lambda` (or else, in a custom \n",
    "layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is the decoder implementation: we reshape the vector `z` to the dimensions of an image, then we use a few convolution layers to obtain a final \n",
    "image output that has the same dimensions as the original `input_img`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the input where we will feed `z`.\n",
    "decoder_input = layers.Input(K.int_shape(z)[1:])\n",
    "\n",
    "# Upsample to the correct number of units\n",
    "x = layers.Dense(np.prod(shape_before_flattening[1:]),\n",
    "                 activation='relu')(decoder_input)\n",
    "\n",
    "# Reshape into an image of the same shape as before our last `Flatten` layer\n",
    "x = layers.Reshape(shape_before_flattening[1:])(x)\n",
    "\n",
    "# We then apply then reverse operation to the initial\n",
    "# stack of convolution layers: a `Conv2DTranspose` layers\n",
    "# with corresponding parameters.\n",
    "x = layers.Conv2DTranspose(32, 3,\n",
    "                           padding='same', activation='relu',\n",
    "                           strides=(2, 2))(x)\n",
    "x = layers.Conv2D(1, 3,\n",
    "                  padding='same', activation='sigmoid')(x)\n",
    "# We end up with a feature map of the same size as the original input.\n",
    "\n",
    "# This is our decoder model.\n",
    "decoder = Model(decoder_input, x)\n",
    "\n",
    "# We then apply it to `z` to recover the decoded `z`.\n",
    "z_decoded = decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual loss of a VAE doesn't fit the traditional expectation of a sample-wise function of the form `loss(input, target)`. Thus, we set up \n",
    "the loss by writing a custom layer with internally leverages the built-in `add_loss` layer method to create an arbitrary loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVariationalLayer(keras.layers.Layer):\n",
    "\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        kl_loss = -5e-4 * K.mean(\n",
    "            1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We don't use this output.\n",
    "        return x\n",
    "\n",
    "# We call our custom layer on the input and the decoded output,\n",
    "# to obtain the final model output.\n",
    "y = CustomVariationalLayer()([input_img, z_decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we instantiate and train the model. Since the loss has been taken care of in our custom layer, we don't specify an external loss \n",
    "at compile time (`loss=None`), which in turns means that we won't pass target data during training (as you can see we only pass `x_train` \n",
    "to the model in `fit`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 32)   320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 14, 14, 64)   18496       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 14, 14, 64)   36928       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 14, 14, 64)   36928       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 12544)        0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           401440      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            66          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            66          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2)            0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 28, 28, 1)    56385       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "custom_variational_layer_1 (Cus [(None, 28, 28, 1),  0           input_1[0][0]                    \n",
      "                                                                 model_1[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 550,629\n",
      "Trainable params: 550,629\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 293s 5ms/step - loss: 0.5894 - val_loss: 0.1981\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 312s 5ms/step - loss: 0.1942 - val_loss: 0.1935\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 327s 5ms/step - loss: 0.1893 - val_loss: 0.1913\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 345s 6ms/step - loss: 0.1866 - val_loss: 0.1852\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 314s 5ms/step - loss: 0.1849 - val_loss: 0.1866\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 314s 5ms/step - loss: 0.1837 - val_loss: 0.1837\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 299s 5ms/step - loss: 0.1832 - val_loss: 0.1820\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 295s 5ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 334s 6ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 303s 5ms/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb86b499780>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "vae = Model(input_img, y)\n",
    "vae.compile(optimizer='rmsprop', loss=None)\n",
    "vae.summary()\n",
    "\n",
    "# Train the VAE on MNIST digits\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape(x_test.shape + (1,))\n",
    "\n",
    "vae.fit(x=x_train, y=None,\n",
    "        shuffle=True,\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once such a model is trained -- e.g. on MNIST, in our case -- we can use the `decoder` network to turn arbitrary latent space vectors into \n",
    "images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/matplotlib/colors.py:897: UserWarning: Warning: converting a masked element to nan.\n",
      "  dtype = np.min_scalar_type(value)\n",
      "/home/farid/.local/lib/python3.6/site-packages/numpy/ma/core.py:716: UserWarning: Warning: converting a masked element to nan.\n",
      "  data = np.array(a, copy=False, subok=subok)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb8233df278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# Linearly spaced coordinates on the unit square were transformed\n",
    "# through the inverse CDF (ppf) of the Gaussian\n",
    "# to produce values of the latent variables z,\n",
    "# since the prior of the latent space is Gaussian\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = decoder.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid of sampled digits shows a completely continuous distribution of the different digit classes, with one digit morphing into another \n",
    "as you follow a path through latent space. Specific directions in this space have a meaning, e.g. there is a direction for \"four-ness\", \n",
    "\"one-ness\", etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
