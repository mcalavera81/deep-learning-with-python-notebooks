{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farid/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "seq_model = Sequential()                                               \n",
    "seq_model.add(layers.Dense(32, activation='relu', input_shape=(64,)))\n",
    "seq_model.add(layers.Dense(32, activation='relu'))\n",
    "seq_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "input_tensor = Input(shape=(64,))                                      \n",
    "x = layers.Dense(32, activation='relu')(input_tensor)                  \n",
    "x = layers.Dense(32, activation='relu')(x)                             \n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)              \n",
    "\n",
    "model = Model(input_tensor, output_tensor)                             \n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  2080\n"
     ]
    }
   ],
   "source": [
    "print(\"Params: \" ,64*32 +32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  1056\n"
     ]
    }
   ],
   "source": [
    "print(\"Params: \" ,32*32 +32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  330\n"
     ]
    }
   ],
   "source": [
    "print(\"Params: \" ,32*10 +10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 11.6957\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 22us/step - loss: 11.6759\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 26us/step - loss: 11.6675\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 56us/step - loss: 11.6609\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 23us/step - loss: 11.6545\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 34us/step - loss: 11.6495\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 22us/step - loss: 11.6453\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 39us/step - loss: 11.6410\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 29us/step - loss: 11.6372\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 30us/step - loss: 11.6336\n",
      "100/100 [==============================] - 0s 362us/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')      \n",
    "import numpy as np                                                       \n",
    "x_train = np.random.random((100, 64))\n",
    "y_train = np.random.random((100, 10))\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128)                   \n",
    "\n",
    "score = model.evaluate(x_train, y_train)                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-answering model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "text_vocabulary_size = 10000\n",
    "question_vocabulary_size = 10000\n",
    "answer_vocabulary_size = 500\n",
    "\n",
    "text_input = Input(shape=(None,), dtype='int32', name='text')            \n",
    "\n",
    "embedded_text = layers.Embedding(\n",
    "    text_vocabulary_size,64)(text_input)                                \n",
    "\n",
    "encoded_text = layers.LSTM(32)(embedded_text)                            \n",
    "\n",
    "question_input = Input(shape=(None,),\n",
    "                       dtype='int32',\n",
    "                       name='question')                                  \n",
    "\n",
    "embedded_question = layers.Embedding(\n",
    "    question_vocabulary_size,32)(question_input)\n",
    "encoded_question = layers.LSTM(16)(embedded_question)\n",
    "\n",
    "concatenated = layers.concatenate([encoded_text, encoded_question],\n",
    "                                  axis=-1)                               \n",
    "\n",
    "answer = layers.Dense(answer_vocabulary_size,\n",
    "                      activation='softmax')(concatenated)                \n",
    "\n",
    "model = Model([text_input, question_input], answer)                      \n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((3,4))\n",
    "print(np.random.randint(0, x.shape[1]-1, size=x.shape[0]))\n",
    "x[np.arange(x.shape[0]),np.random.randint(0, x.shape[1]-1, size=x.shape[0])] = 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 6.3545 - acc: 1.0000e-03\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 6.1810 - acc: 0.0020\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 6.1200 - acc: 0.0050\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 6.0173 - acc: 0.0060\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 5.9046 - acc: 0.0100\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 5.7733 - acc: 0.0150\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 5.6802 - acc: 0.0200\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 5.5430 - acc: 0.0260\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 5.4542 - acc: 0.0210\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 5.3843 - acc: 0.0300\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 5.3042 - acc: 0.0470\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 5.2743 - acc: 0.0430\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 5.1745 - acc: 0.0610\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 5.1128 - acc: 0.0610\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 5.0601 - acc: 0.0690\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 5.0255 - acc: 0.0770\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.9579 - acc: 0.0770\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.9154 - acc: 0.0910\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.8497 - acc: 0.0900\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.8083 - acc: 0.0990\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.7622 - acc: 0.0940\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.7146 - acc: 0.0980\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.6737 - acc: 0.1130\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.6420 - acc: 0.1130\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.6109 - acc: 0.1180\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.5444 - acc: 0.1300\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.5043 - acc: 0.1370\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.5051 - acc: 0.1390\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.4354 - acc: 0.1520\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.3964 - acc: 0.1500\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 997us/step - loss: 4.3736 - acc: 0.1570\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.3243 - acc: 0.1650\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.2743 - acc: 0.1780\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.2318 - acc: 0.1890\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.2099 - acc: 0.1980\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.1801 - acc: 0.1970\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.1413 - acc: 0.2060\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 4.0931 - acc: 0.2200\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 4.0583 - acc: 0.2220\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 4.0064 - acc: 0.2320\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.9706 - acc: 0.2310\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.9381 - acc: 0.2510\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.9178 - acc: 0.2600\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.8601 - acc: 0.2570\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.8445 - acc: 0.2660\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.7793 - acc: 0.2850\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.7389 - acc: 0.2760\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.7045 - acc: 0.2940\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.6498 - acc: 0.3120\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 3.5933 - acc: 0.3220\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.5801 - acc: 0.3270\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.5487 - acc: 0.3300\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.4722 - acc: 0.3390\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.4308 - acc: 0.3560\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.4030 - acc: 0.3580\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.3482 - acc: 0.3590\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.3025 - acc: 0.3840\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.2558 - acc: 0.3940\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.2179 - acc: 0.3860\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.1402 - acc: 0.4120\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.1292 - acc: 0.4090\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.0599 - acc: 0.4310\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 3.0164 - acc: 0.4330\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.9607 - acc: 0.4480\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.9247 - acc: 0.4580\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.8974 - acc: 0.4680\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.8160 - acc: 0.4810\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.7596 - acc: 0.5000\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 992us/step - loss: 2.7165 - acc: 0.4980\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.6897 - acc: 0.5230\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.6257 - acc: 0.5350\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.5785 - acc: 0.5390\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.5888 - acc: 0.5350\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.5139 - acc: 0.5660\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.4629 - acc: 0.5720\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 1s 984us/step - loss: 2.4626 - acc: 0.5820\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3725 - acc: 0.6070\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.3200 - acc: 0.6320\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.2752 - acc: 0.6440\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.2384 - acc: 0.6500\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.1691 - acc: 0.6790\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.1160 - acc: 0.6870\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.1141 - acc: 0.6950\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 2.0477 - acc: 0.7100\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.9955 - acc: 0.7270\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.9594 - acc: 0.7360\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.9354 - acc: 0.7520\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.8674 - acc: 0.7560\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.8281 - acc: 0.7800\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.7883 - acc: 0.7730\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.7490 - acc: 0.7970\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.7000 - acc: 0.8190\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.7145 - acc: 0.7970\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.6910 - acc: 0.8040\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.5999 - acc: 0.8230\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.5544 - acc: 0.8400\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.5065 - acc: 0.8550\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.4762 - acc: 0.8550\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.4315 - acc: 0.8710\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.3997 - acc: 0.8770\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.3703 - acc: 0.8780\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.3522 - acc: 0.8780\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.3327 - acc: 0.8870\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.3510 - acc: 0.8780\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.2297 - acc: 0.9070\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.1862 - acc: 0.9210\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.1720 - acc: 0.9210\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.1344 - acc: 0.9280\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.0996 - acc: 0.9330\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.0802 - acc: 0.9280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe65e9926a0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = 1000\n",
    "max_length = 100\n",
    "\n",
    "text = np.random.randint(1, text_vocabulary_size,\n",
    "                         size=(num_samples, max_length))                   \n",
    "question = np.random.randint(1, question_vocabulary_size,\n",
    "                             size=(num_samples, max_length))\n",
    "\n",
    "answers = np.zeros((num_samples,answer_vocabulary_size))\n",
    "answers[np.arange(answers.shape[0]),np.random.randint(0, answers.shape[1], size=answers.shape[0])] = 1\n",
    "\n",
    "model.fit([text, question], answers, epochs=10, batch_size=128)            \n",
    "\n",
    "model.fit({'text': text, 'question': question}, answers,                   \n",
    "          epochs=100, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three-output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/farid/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "vocabulary_size = 50000\n",
    "num_income_groups = 10\n",
    "\n",
    "posts_input = Input(shape=(None,), dtype='int32', name='posts')\n",
    "embedded_posts = layers.Embedding(vocabulary_size, 256)(posts_input)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_posts)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "age_prediction = layers.Dense(1, name='age')(x)               \n",
    "income_prediction = layers.Dense(num_income_groups,\n",
    "                                 activation='softmax',\n",
    "                                 name='income')(x)\n",
    "gender_prediction = layers.Dense(1, activation='sigmoid', name='gender')(x)\n",
    "\n",
    "model = Model(posts_input,\n",
    "              [age_prediction, income_prediction, gender_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'])\n",
    "\n",
    "model.compile(optimizer='rmsprop',                             \n",
    "              loss={'age': 'mse',                              \n",
    "                    'income': 'categorical_crossentropy',      \n",
    "                    'gender': 'binary_crossentropy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'],\n",
    "              loss_weights=[0.25, 1., 10.])\n",
    "\n",
    "model.compile(optimizer='rmsprop',                            \n",
    "              loss={'age': 'mse',                             \n",
    "                    'income': 'categorical_crossentropy',     \n",
    "                    'gender': 'binary_crossentropy'},         \n",
    "              loss_weights={'age': 0.25,                      \n",
    "                            'income': 1.,                     \n",
    "                            'gender': 10.})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: This function is deprecated. Please call randint(0, 1 + 1) instead\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 5963463.3640 - age_loss: 2122.3955 - income_loss: 5962900.2560 - gender_loss: 3.2475\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 16s 16ms/step - loss: 6570802.6240 - age_loss: 823.1293 - income_loss: 6570576.9880 - gender_loss: 1.9846\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 20s 20ms/step - loss: 6570792.3360 - age_loss: 791.1411 - income_loss: 6570576.9760 - gender_loss: 1.7661\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: 6570810.2200 - age_loss: 848.3705 - income_loss: 6570576.8960 - gender_loss: 2.1191\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: 6570800.5120 - age_loss: 806.3608 - income_loss: 6570576.9840 - gender_loss: 2.1932\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 6570811.3880 - age_loss: 833.1162 - income_loss: 6570576.9240 - gender_loss: 2.6179\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 19s 19ms/step - loss: 6571218.2840 - age_loss: 2457.5334 - income_loss: 6570576.8320 - gender_loss: 2.7018\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: 6572235.5720 - age_loss: 6404.5126 - income_loss: 6570576.9560 - gender_loss: 5.7434\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: 6751815.2560 - age_loss: 724655.9305 - income_loss: 6570577.0280 - gender_loss: 7.4225\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 16s 16ms/step - loss: 15024036.6240 - age_loss: 33831494.5823 - income_loss: 6566085.9800 - gender_loss: 7.7206\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 16s 16ms/step - loss: 6570911.3640 - age_loss: 1029.1767 - income_loss: 6570576.9200 - gender_loss: 7.7206\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 16s 16ms/step - loss: 13166265.1520 - age_loss: 26382443.7521 - income_loss: 6570576.9880 - gender_loss: 7.7206\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 18s 18ms/step - loss: 7899870.6800 - age_loss: 5316865.9878 - income_loss: 6570576.9240 - gender_loss: 7.7206\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: 31791409.0880 - age_loss: 100883019.1151 - income_loss: 6570576.9680 - gender_loss: 7.7206\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: 6572724.6120 - age_loss: 8282.0108 - income_loss: 6570576.9680 - gender_loss: 7.7206\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 24834505.9520 - age_loss: 73066422.8880 - income_loss: 6567822.5200 - gender_loss: 7.7206\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: 6791646.9200 - age_loss: 883971.3185 - income_loss: 6570576.9200 - gender_loss: 7.7206\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: 7604805.8360 - age_loss: 4136606.3120 - income_loss: 6570577.0240 - gender_loss: 7.7206\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 18s 18ms/step - loss: 37625931.6360 - age_loss: 124221103.1118 - income_loss: 6570577.0280 - gender_loss: 7.9714\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 18s 18ms/step - loss: 6571084.5160 - age_loss: 1721.5418 - income_loss: 6570576.9800 - gender_loss: 7.7206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe65e9bf4a8>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = 1000\n",
    "max_length = 500\n",
    "posts = np.random.randint(1, vocabulary_size,\n",
    "                         size=(num_samples, max_length))                   \n",
    "\n",
    "age_targets = np.random.randint(1, 99,size=(num_samples))                   \n",
    "\n",
    "\n",
    "income_targets = np.random.randint(1, 100000,size=(num_samples,num_income_groups))\n",
    "income_targets[np.arange(income_targets.shape[0]),np.random.randint(0, income_targets.shape[1], size=income_targets.shape[0])] = 1\n",
    "\n",
    "\n",
    "gender_targets =np.random.random_integers(0, 1,size=(num_samples))\n",
    "\n",
    "model.fit(posts, [age_targets, income_targets, gender_targets],        \n",
    "          epochs=10, batch_size=64)\n",
    "\n",
    "model.fit(posts, {'age': age_targets,                                  \n",
    "                  'income': income_targets,                            \n",
    "                  'gender': gender_targets},                           \n",
    "          epochs=10, batch_size=64) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "\n",
    "x = Input(shape=(128,128,3), dtype='float32')\n",
    "branch_a = layers.Conv2D(128, 1,\n",
    "                         activation='relu', strides=2)(x)                  \n",
    "branch_b = layers.Conv2D(128, 1, activation='relu')(x)                     \n",
    "branch_b = layers.Conv2D(128, 3, activation='relu', strides=2, padding='same')(branch_b)   \n",
    "\n",
    "branch_c = layers.AveragePooling2D(3, strides=2, padding='same')(x)                        \n",
    "branch_c = layers.Conv2D(128, 3, activation='relu', padding='same')(branch_c)              \n",
    "\n",
    "branch_d = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu', padding='same')(branch_d)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu', strides=2, padding='same')(branch_d)\n",
    "output = layers.concatenate(\n",
    "    [branch_a, branch_b, branch_c, branch_d], axis=-1)                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import layers\n",
    "\n",
    "# x = ...\n",
    "# y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)    \n",
    "# y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "# y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "\n",
    "# y = layers.add([y, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import layers\n",
    "\n",
    "# x = ...\n",
    "# y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "# y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "# y = layers.MaxPooling2D(2, strides=2)(y)\n",
    "\n",
    "# residual = layers.Conv2D(128, 1, strides=2, padding='same')(x)       1\n",
    "\n",
    "# y = layers.add([y, residual]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Siamese LSTM model or a shared LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "\n",
    "lstm = layers.LSTM(32)                                                \n",
    "left_input = Input(shape=(None, 128))                                 \n",
    "left_output = lstm(left_input)                                        \n",
    "\n",
    "right_input = Input(shape=(None, 128))                                \n",
    "right_output = lstm(right_input)                                      \n",
    "\n",
    "merged = layers.concatenate([left_output, right_output], axis=-1)     \n",
    "predictions = layers.Dense(1, activation='sigmoid')(merged)           \n",
    "\n",
    "model = Model([left_input, right_input], predictions)                 \n",
    "# model.fit([left_data, right_data], targets)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Siamese vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import applications\n",
    "from keras import Input\n",
    "\n",
    "xception_base = applications.Xception(weights=None,\n",
    "                                      include_top=False)      1\n",
    "\n",
    "left_input = Input(shape=(250, 250, 3))                       2\n",
    "right_input = Input(shape=(250, 250, 3))                      2\n",
    "\n",
    "left_features = xception_base(left_input)                     3\n",
    "right_input = xception_base(right_input)                      3\n",
    "\n",
    "merged_features = layers.concatenate(\n",
    "    [left_features, right_input], axis=-1)                    4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
